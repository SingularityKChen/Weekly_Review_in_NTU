---
layout: post
title: "2020/01/06-12 weekly review"
description: "the weekly review from 2020/01/06 to 12"
categories: [WeeklyReview, ReadPaper]
tags: [WeeklyReview, EIE, NAS, CSC, Chisel, Scala, DesignAutomation]
last_updated: 2020-02-16 19:14:00 GMT+8
excerpt: This review contains some notes of four papers written by Han Song 1) EIE" Efficient Inference Engine on Compressed Deep Neural Network; 2) ProxylessNAS" Direct Neural Architecture Search on Target Task and Hardware; 3) Design Automation for Efficient Deep Learning Computing; 4) Learning to Design Circuits. Also, there are some Chisel and Scala syntaxes studying notes.
redirect_from:
  - /2020/01/12/
---

* Kramdown table of contents
{:toc .toc}
# 2020/01/06-12

This week I read several papers written by Song Han including [reference](https://github.com/SingularityKChen/Weekly_Review_in_NTU/blob/master/Reference.md) 12, 15-17, related to both deep learning accelerator architecture with Weight stationery and design automation for Neural Architecture Search.

Also, I began the implementation of [Processing Element](https://github.com/SingularityKChen/dl_accelerator), and finished the main function without any test.

So next week, I plan to finish the PE with a test file and then establish the analysis system based on [Eyexam](#eyexam) with some scripts to compute the details of the hardware. So I need to re-read the papers related to [Eyeriss](#20191223-29).

---

## EIE: Efficient Inference Engine on Compressed Deep Neural Network

![Efficient inference engine that works on the compressed deep neural network model](https://images-cdn.shimo.im/7hWMl33GZigEvWNW/image.png)

### Exploit the Sparsity of Activations with Compressed Sparse Column (CSC) Format

For each column $W_j$ of the matrix $W$ we store a vector $v$ that contains the non-zero weights, and a second, equal-length vector $z$ that encodes the number of zeros before the corresponding entry in $v$. Each entry of  $v$ and $z$ is represented by a four-bit value. If more than 15 zeros appear
before a non-zero entry we add a zero in vector $v$.

![Memory layout for the relative indexed](https://images-cdn.shimo.im/2LMlD7Bu84UsueOG/image.png)

### Parallelizing Compressed DNN

perform the sparse matrix sparse vector operation by scanning vector $a$ to find its next non-zero value  $a_j$and broadcasting $a_j$ along with its index $j$ to all PEs. Each PE then multiplies $a_j$ by the non-zero elements in its portion of column $W_j$ — accumulating the partial sums in accumulators for each element of the output activation vector b. In the CSC representation, these non-zeros weights are stored contiguously so each PE simply walks through its $v$ array from location $p_j$ to $p_{j+1}-1$ to load the weights. To address the output accumulators, the row number $i$ corresponding to each weight $W_{ij}$ is generated by keeping a running sum of the entries of the x array.

![](https://images-cdn.shimo.im/8Y6M5PoCqV01NZdy/image.png)

The interleaved CSC representation allows each PE to quickly find the non-zeros in each column to be multiplied by $a_j$. This organization also keeps all of the computation except for the broadcast of the input activations local to a PE.

### Hardware Implementation

![](https://images-cdn.shimo.im/Nr3mEv6LW0Y31KxB/image.png)

+ Activation Queue and Load Balancing: The broadcast is disabled if any PE has a full queue. At any point in time, each PE processes the activation at the head of its queue.
+ Pointer Read Unit: $p_j$ and $p_{j+1}$ will always be in different banks.
+ Sparse Matrix Read Unit: uses pointers $p_j$ and $p_{j+1}$ to read the non-zero elements
+ Arithmetic Unit: performs the multiply-accumulate operation
+ Activation Read/Write: contains two activation register files that accommodate the source and destination activation values respectively during a single round of FC layer computation
+ Distributed Leading Non-Zero Detection: select the first non-zero result. The result is sent to a Leading Nonzero Detection Node (LNZD Node)
+ Central Control Unit: It communicates with the master and monitors the state of every PE by setting the control registers. There are two modes in the Central Unit: I/O and Computing.
  +  In the I/O mode, all of the PEs are idle while the activations and weights in every PE can be accessed by a DMA connected with the Central Unit. This is a one-time cost. 
     + In the Computing mode, the CCU repeatedly collects a non-zero value from the LNZD quadtree and broadcasts this value to all PEs.

brought the critical path delay down to 1.15ns by introducing 4 pipeline stages to update one activation:

+ codebook lookup and address accumulation (in parallel)
+ output activation read and input activation multiply (in parallel)
+ shift and add
+ output activation write.

### Design Space Exploration

**Queue Depth**. The activation FIFO queue deals with load imbalance between the PEs. A deeper FIFO queue can better decouple producer and consumer.

**SRAM Width**. We choose an SRAM with a 64-bit interface to store the sparse matrix (`Spmat`) since it minimized the total energy. Wider SRAM interfaces reduce the number of total SRAM accesses but increase the energy cost per SRAM read.

**Arithmetic Precision**. We use a 16-bit fixed-point arithmetic. 16-bit fixed-point multiplication consumes 5 times less energy than 32-bit fixed-point and 6.2 times less energy than 32-bit floating-point.

## ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware

ProxylessNAS is the first NAS algorithm that directly learns architectures on the largescale dataset (e.g. ImageNet) without any proxy while still allowing a large candidate set and removing the restriction of repeating blocks. it is the first work to study specialized neural network architectures for different hardware architectures.

![optimizes neural network architectures](https://images-cdn.shimo.im/qiQowKgCctgUyfLd/image.png)

### Method

+ describe the construction of the over-parameterized network with all candidate paths
+ introduce how we leverage binarized architecture parameters to reduce the memory consumption of training the over-parameterized network to the same level as regular training.

![Learning both weight parameters and binarized architecture parameters](https://images-cdn.shimo.im/GtQyHdVEM7UiZmh7/image.png)

+ first freeze the architecture parameters and stochastically sample binary gates according to Eq. (2) for each batch of input data
+ the weight parameters of active paths are updated via standard gradient descent on the training set (Figure 2 left)
+ When training architecture parameters, the weight parameters are frozen
+ reset the binary gates and update the architecture parameters on the validation set (Figure 2 right). These two update steps are performed in an alternative manner. 
+ derive the compact architecture by pruning redundant paths

### Make Latency Differentiable

Model the latency of a network as a continuous function of the neural network dimensions.

![Making latency differentiable by introducing latency regularization loss](https://images-cdn.shimo.im/4wwTe3kgMO4AJleJ/image.png)

## Design Automation for Efficient Deep Learning Computing

![Design automation for model specialization, channel pruning and mixed-precision quantization](https://images-cdn.shimo.im/anETjNuh440NbKou/image.png)

Three main points in this paper:

+ automatically designing specialized fast models
+ auto channel pruning
+ auto mixed-precision quantization

### Automated Model Specialization

To fully utilize the hardware resource, start with a large design space (Figure 1(a)) that includes many candidate paths to learn which is the best one by gradient descent, rather than hand-picking with rule-based heuristics.

The search space for each block $i$ consists of many choices:

+ `ConvOp`: mobile inverted bottleneck conv [9] with various kernel sizes and expansion ratios
  + Kernel size: {3\*3, 5\*5, 7\*7}
    + Expansion ratio: {3, 6}
+ `ZeroOp`: if `ZeroOp` is chosen at $i^{th}$ block, it means the block is skipped.

In the forward step, to save GPU memory, we allow only one candidate path to actively reside in the GPU memory. This is achieved by hard-thresholding the probability of each candidate path to either 0 or 1 (i.e., path-level binarization).

### Automated Channel Pruning

Pruning too much will hurt accuracy; too less will not achieve high compression ratio.

Our automatic model compression (AMC) leverages reinforcement learning to efficiently search the pruning ratio.

We train a reinforcement learning agent to predict the best sparsity for a given hardware. We evaluate the accuracy and FLOPs after pruning. Then we update the agent by encouraging smaller, faster and more accurate models.

The easiest way to reduce the channels of a model is to use uniform channel shrinkage, i.e. use a width multiplier to uniformly reduce the channels of each layer with a fixed ratio.

### Automated Mixed-Precision Quantization

Our hardware-aware automatic quantization (HAQ) models the quantization task as a reinforcement learning problem. We use the actor-critic model to give the quantization policy (#bits per layer) (Figure 1(c)). The goal is not only high accuracy but also low energy and low latency.

Inferencing on edge devices and cloud servers can be quite different, since:

+ the batch size on the cloud servers are larger
+ the edge devices are usually limited to low computation resources and memory bandwidth.

Interpret the quantization policy’s difference between edge and cloud by the roofline model.

## Learning to Design Circuits

Two difficult for searching for parameters that satisfy circuit specifications due to the low availability of training data:

+ Circuit simulation is slow, thus generating large-scale dataset is time-consuming
+ Most circuit designs are propitiatory IPs within individual IC companies, making it expensive to collect large-scale datasets

Constrains for L2DC(Learning to Design Circuits): 

+ meet hard-constraints (eg. gain, bandwidth)

+ optimize good-to-have targets (eg. area, power)

![Learning to Design Circuits (L2DC) Method Overview](https://images-cdn.shimo.im/DygvZ9uF87sLxhUy/image.png)

Steps:

+ leverages reinforcement learning (RL) to generate circuits data by itself and learns from the data to search for best parameters.

+ produces an action (a set of parameters) to the circuit simulator environment, and then receives a reward as a function of gain, bandwidth, power, area, etc.
+ The reward is defined to optimize the desired Figures of Merits (FOM) composed of several performance metrics.
+ By maximizing the reward, RL agent can optimize the circuit parameters.

![use sequence to sequence model to generate circuit parameters](https://images-cdn.shimo.im/AY5WaAFy94ct6a5z/image.png)

## Chisel & Scala Syntax

Useful resource:

Chisel [Wiki](https://github.com/freechipsproject/chisel3/wiki/CS250-Chisel+Scala-Primer)

Chisel [CookBook](https://github.com/freechipsproject/chisel3/wiki/Cookbook)

Chisel3 [API](https://www.chisel-lang.org/api/latest/chisel3/index.html)

[Runoob Scala tutorial](https://www.runoob.com/scala/scala-tutorial.html)

Scala language [PDF](https://static.runoob.com/download/Scala%E8%AF%AD%E8%A8%80%E8%A7%84%E8%8C%83.pdf)

### `Chisel.Queue`

```scala
val qa = Queue(io.a) // io.a is the input to the FIFO
																				 // qa is DecoupledIO output from FIFO

```



### `Chisel.Decoupled`

```scala
val b = Flipped(Decoupled(UInt(32.W)))// valid and bits are inputs
val z = Decoupled(UInt(32.W)) // valid and bits are outputs
```



### `Chisel.suggestName`

```scala
		def inc(x: UInt): UInt = {
				val incremented = x + 1.U // We cannot name this, it's inside a method
				incremented.suggestName("incremented") // Now it is named!
		}
```



### `Chisel.print`

```scala
println(s"${io.in}") 
// chisel3.core.UInt@e

printf(p"$io") 
// Bundle(in->3, out->3)

println(s"out = ${peek(c.io.out)}") 
// out = Vector(0, 0, 0, 0, 0, 0)
```



### `Scala.zip`

Pair up values in two lists.

```scala
scala> List("a", "b", "c").zipWithIndex
res0: List[(String, Int)] = List((a,0), (b,1), (c,2))

scala> List("a", "b", "c") zip (Stream from 1)
res1: List[(String, Int)] = List((a,1), (b,2), (c,3))

scala> List(1, 2, 3).zip(List("a", "b", "c"))
res2: List[(Int, String)] = List((1,a), (2,b), (3,c))
```



### `Scala.map`

Applies a function to each element in a list and returns the resulting transformation in a list

```scala
scala> val list1 = 0 to 5 toList
res3: List[Int] = List(0, 1, 2, 3, 4, 5)
 
scala> list1.map(x=>x*x)
res4: List[Int] = List(0, 1, 4, 9, 16, 25)
```



### `Scala.reduce` (`reduceLeft` & `reduceRight`)

In each step, a function is performed on the list element and the result of the last fold operation, returning a single value.

```scala
scala> val list1 = 0 to 5 toList
res51: List[Int] = List(1, 2, 3, 4, 5)

scala> val sum = (x:Int, y:Int) => {println(x,y) ; x + y}
sum: (Int, Int) => Int = <function2>

scala> list1.reduce(sum)
(1,2) // 1 + 2 =  3, list1 = List(3, 3, 4, 5)
(3,3) // 3 + 3 =  6, list1 = List(6, 4, 5)
(6,4) // 6 + 4 = 10, list1 = List(10, 5)
(10,5)
res52: Int = 15
```



### `Scala.List.take(Int)`

See [Scala List](https://www.runoob.com/scala/scala-lists.html).

```scala
val mcrenf = RegInit(VecInit(Seq.fill(6)(0.U(loop_width.W))))
val when_carry: IndexedSeq[Bool] = mcrenf.zip(MCRENF.map(x=> x - 1)).map{ case (x,y) => x === y.U}

when_carry.take(i).reduce(_ && _)
```



### Reg Delay Test

```scala
class TheOperModule extends Module {
		val io = IO(new Bundle {
				val a  = Input(UInt(4.W))
				val b  = Input(UInt(4.W))
				val out_reg0 = Output(UInt(4.W))
				val out_reg1 = Output(UInt(4.W))
				val out_reg2 = Output(UInt(4.W))
		})

				val a_reg = RegInit(0.U(4.W))
				val b_reg = RegInit(0.U(4.W))
				val c_reg1 = RegInit(0.U(4.W))
				val d_reg2 = RegInit(0.U(4.W))
				a_reg := io.a
				b_reg := io.b
				c_reg1 := io.a + io.b
				d_reg2 := a_reg + b_reg
		
				io.out_reg0 := io.a + io.b
				io.out_reg1 := c_reg1
				io.out_reg2 := d_reg2
}
class MyOperTester(c: TheOperModule) extends PeekPokeTester(c) {
				val in_a = 2
				val in_b = 3
				poke(c.io.a, in_a)
				poke(c.io.b, in_b)
				println(s"clock 0, out_reg0 = ${peek(c.io.out_reg0)}")
				println(s"         out_reg1 = ${peek(c.io.out_reg1)}")
				println(s"         out_reg2 = ${peek(c.io.out_reg2)}")
				step(1)
				println(s"clock 1, out_reg0 = ${peek(c.io.out_reg0)}")
				println(s"         out_reg1 = ${peek(c.io.out_reg1)}")
				println(s"         out_reg2 = ${peek(c.io.out_reg2)}")
				step(1)
				println(s"clock 2, out_reg0 = ${peek(c.io.out_reg0)}")
				println(s"         out_reg1 = ${peek(c.io.out_reg1)}")
				println(s"         out_reg2 = ${peek(c.io.out_reg2)}")
}
assert(Driver(() => new TheOperModule) {c => new MyOperTester(c)})
```

```pseudocode
[info] [0.001] clock 0, out_reg0 = 5
[info] [0.001]          out_reg1 = 0
[info] [0.001]          out_reg2 = 0
[info] [0.001] clock 1, out_reg0 = 5
[info] [0.001]          out_reg1 = 5 // arrive reg 1
[info] [0.002]          out_reg2 = 0
[info] [0.002] clock 2, out_reg0 = 5
[info] [0.002]          out_reg1 = 5
[info] [0.002]          out_reg2 = 5 // arrive reg 2
```