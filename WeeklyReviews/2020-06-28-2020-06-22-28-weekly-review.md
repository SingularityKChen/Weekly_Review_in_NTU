---
layout: post
title: "[Weekly Review] 2020/06/22-28"
description: "The weekly review from 2020/06/22 to 28"
categories: [WeeklyReview]
tags: [Category]
last_updated: 2020-06-29 13:56:00 GMT+8
excerpt: 
redirect_from:
  - /2020/06/08/
---

* Kramdown table of contents
{:toc .toc}
# 2020/06/22-28

This week, I read two and a half papers:

+ A domain-specific supercomputer for training deep neural networks
+ In-Datacenter Performance Analysis of a Tensor Processing Unit
+ Hyperdimensional Computing: An Introduction to Computing in Distributed Representation with High-Dimensional Random Vectors (unfinished)

And I also learned several terms (see posts bellow) as well as a little SystemC syntax.

Additionally, I watched the presentation video of `Deep Learning Hardware: Past, Present, and Future` via YouTube.

I posted these blogs this week (most of them are posted on Sunday):

+ [Round-Robin Arbitration](https://singularitykchen.github.io/blog/2020/06/25/Glean-Round-Robin-Arbitration/): a scheduling scheme

+ [Unified Power Format](https://singularitykchen.github.io/blog/2020/06/25/Glean-Unified-Power-Format/): The Unified Power Format (UPF) is intended to ease the job of specifying, simulating and verifying IC designs that have a number of power states and power islands.

+ [All-Reduce Operations](https://singularitykchen.github.io/blog/2020/06/28/Glean-All-Reduce-Operations/): one kind of collective operations in NCCL and MPI lib

+ [Operator Fusion](https://singularitykchen.github.io/blog/2020/06/28/Glean-Operator-Fusion/): fuse chains of basic operators

+ [A domain-specific supercomputer for training deep neural networks](https://singularitykchen.github.io/blog/2020/06/28/Read-Paper-A-domain-specific-supercomputer-for-training-DNN/): 

+ [Deep Learning Hardware: Past, Present, and Future](https://singularitykchen.github.io/blog/2020/06/28/Read-Paper-Deep-Learning-Hardware-Past-Present-and-Future/): 

+ [TEA-DNN: the Quest for Time-Energy-Accuracy Co-optimized Deep Neural Networks](https://singularitykchen.github.io/blog/2020/06/28/Read-Paper-TEA-DNN-the-Quest-for-Time-Energy-Accuracy-Co-optimized-DNN/): 

+ [tinyML Talks: Low-Power Computer Vision](https://singularitykchen.github.io/blog/2020/06/28/Workshop-Low-Power-Computer-Vision/):  introduction of hierarchical neural network

+ [tinyML Talks: Saving 95% of Your Edge Power with Sparsity](https://singularitykchen.github.io/blog/2020/06/28/Workshop-Saving-95-of-your-edge-power-with-Sparsity/): It will explain these types of sparsity (time, space,  connectivity, activation) in terms of edge processes, and how they  affect computation on a practical level.

  