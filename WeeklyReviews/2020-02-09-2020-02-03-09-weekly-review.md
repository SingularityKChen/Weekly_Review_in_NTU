---
layout: post
title: "2020/02/03-09 weekly review"
last_updated: 2020-02-10 18:57:00 GMT+8
description: "the weekly review from 2020/02/03 to 09"
categories: [WeeklyReview]
tags: [WeeklyReview, Chisel, BNN, codesign, HPML, HPC]
excerpt: This review contains the usage of general data type in Chisel, the basic architecture of NN and the introductions of BNN and the BitFlow algorithm. Also, one paper related to FPGA and DNN co-design and some materials related to HPC+ML.
redirect_from:
  - /2020/02/09/
---

* Kramdown table of contents
{:toc .toc}
# 2020/02/16-22

## Chisel Syntax

### Transferring data type in generators

```scala
class ClusterCommonCtrlIO[T1:< Data, T2:< Data](dataType1: T1, dataType2: T2) extends Bundle {
  val inDataSel: T1 = Output(dataType1)
  val outDataSel: T2 = Output(dataType2)
}

class IactClusterIO[T1, T2](dataType1: T1, dataType2: T2, portNum: Int, addrWidth: Int, dataWidth: Int, addrLenWidth: Int, dataLenWidth: Int) extends Bundle {
  val dataPath: Vec[ClusterAddrWithDataCommonIO] = Vec(portNum, new ClusterAddrWithDataCommonIO(addrWidth, dataWidth, addrLenWidth, dataLenWidth)) // output bits and valid
  val ctrlPath = new ClusterCommonCtrlIO[T1, T2](dataType1, dataType2)
}

class IactRouterIO extends Bundle with ClusterConfig {
  val outIOs = new IactClusterIO(UInt(2.W), UInt(2.W), iactPortNum, iactAddrWidth, iactDataWidth, commonLenWidth, commonLenWidth)
  val inIOs: IactClusterIO[UInt, UInt] = Flipped(new IactClusterIO(UInt(2.W), UInt(2.W), iactPortNum, iactAddrWidth, iactDataWidth, commonLenWidth, commonLenWidth))
}

class PEAndRouterIO extends Bundle with ClusterConfig {
  val iactCluster: IactClusterIO[Bool, UInt] = Flipped(new IactClusterIO[Bool, UInt](Bool(), UInt(2.W), iactRouterNum, iactAddrWidth, iactDataWidth, commonLenWidth, commonLenWidth))
}
```

## Neural Network

### [Architecture](https://www.youtube.com/watch?v=oJNHXPs0XDk)

![](https://raw.githubusercontent.com/SingularityKChen/PicUpload/master/img/Snipaste_2020-02-05_12-56-42.png)

### [Overview]((https://www.youtube.com/watch?v=aIZtJqtzdQs))

![Autoencoder](https://raw.githubusercontent.com/SingularityKChen/PicUpload/master/img/Snipaste_2020-02-05_13-00-16.png)

### [Types of ML](https://www.youtube.com/watch?v=YlGEQyEM_a8)

![Types of Machine Learning](https://raw.githubusercontent.com/SingularityKChen/PicUpload/master/img/20200206191447Types%20of%20Machine%20Learning.png)

## HPC + AI

### HPC + Ai: Machine Learning Models in Scientific Computing

[This YouTube video](https://www.youtube.com/watch?v=SV3cnWf39kc) and [see the slide here](http://www.hpcadvisorycouncil.com/events/2019/stanford-workshop/pdf/DayTwo_Friday_15Feb_2019/S_Oberlin_HPC+AI_Friday_02152019.pdf).

![INGREDIENTS: BIG DATA](https://raw.githubusercontent.com/SingularityKChen/PicUpload/master/img/202002051919018%20INGREDIENTS%3A%20BIG%20DATA.png)

![](https://raw.githubusercontent.com/SingularityKChen/PicUpload/master/img/20200205192524.png)

![MIIDAPS-AI](https://raw.githubusercontent.com/SingularityKChen/PicUpload/master/img/20200205213247MIIDAPS-AI.png)

![THE PROMISE OF HPC+AI](https://raw.githubusercontent.com/SingularityKChen/PicUpload/master/img/20200205213333THE%20PROMISE%20OF%20HPC%2BAI.png)

### [How machine learning can improve HPC applications](https://blog.surf.nl/en/how-machine-learning-can-improve-hpc-applications/)

Traditionally, the main workloads run on a supercomputer consists of  various forms of numerical simulations, such as weather modelling,  molecular dynamics, etc.

Early results indicate that these models, that combine machine learning  and traditional simulation, can improve accuracy, accelerate time to  solution and significantly reduce costs.

And in that context, is it a pre- or post-processing step to help filter and understand the input data or ultimate simulation results, or is it  something that is poised to (partly) replace the decades-old codes that  comprise many HPC workloads?

Consequently, early research projects have shown that machine learning  often requires orders of magnitude fewer resources to unlock problems  that have often been beyond the grasp of traditional techniques. Since  performance increases in traditional high-performance computing (HPC)  have been highly dependent on Mooreâ€™s Law, this approach presents a  promising avenue to explore.

Use of machine learning to increase the resolution of a traditional  numerical simulation. The traditional simulation would still be used for simulation at the conventional grid size, while the machine learning  model could use that as input to make predictions at an increased  resolution.

![Use of machine learning to increase the resolution of a traditional  numerical simulation. The traditional simulation would still be used for simulation at the conventional grid size, while the machine learning  model could use that as input to make predictions at an increased  resolution.](https://blog.surf.nl/wp-content/uploads/2018/07/Afbeelding1.png)

### [Supercomputers and Machine Learning: A Perfect Match - insideBIGDATA](https://insidebigdata.com/2019/11/27/supercomputers-and-machine-learning-a-perfect-match/)

Unlike traditional computers, supercomputers use parallel processing.  This multitasking capability enables supercomputers to process  calculations at a much faster rate. 

#### What Is High-Performance Computing and How Does it Work?

HPC architectures typically consist of three components: the Compute Cluster, the Network and Data Storage. 

#### To Cloud or Not to Cloud?

Big data is too big to be processed using traditional database  techniques. The cloud provides the scalability and flexibility required  to process such large data sets. Hardware virtualization, for example,  enables organizations to scale easily. This is useful for data-intensive applications. 

However, there are some considerations when moving big data operations  to the cloud. **Such large and varied amounts of data can be problematic  to synch between on-premises data centers and the cloud.** This can affect the I/O performance in the cloud environment.

Scientists are using machine learning techniques to improve the auto-tuning of exascale applications.

High-performance computing provides unique advantages for machine learning models, including:

- Large amounts of floating-point operations (FLOPS)
- Low-latency
- Parallel I/O

## FPGA/DNN Co-Design: An Efficient Design Methodology for IoT Intelligence on the Edge

Develop DNNs and the corresponding FPGA accelerators simultaneously. DNN designs should be FPGA-architecture driven, and FPGA accelerators should be DNN-aware.

### Contributions

- simultaneous FPGA/DNN co-design methodology with
  - hardware-oriented DNN model design following **bottom-up approach**;
  - DNN-driven FPGA accelerator design following **top-down approach**.
- For DNN model design, we introduce a DNN template to guide the DNN generation with **predictable performance** and **resource utilization**, which greatly reduces the co-design search space. Based on such template, an automatic DNN model search engine, `Auto-DNN`, is proposed to effectively explore the design space and generate DNN models for desired QoR.
- For FPGA accelerator design, we introduce a fine-grained tile-based pipeline architecture, which supports arbitrary DNNs generated by `Auto-DNN` using a library of highly optimized HLS IPs. Based on such architecture, an automatic HLS generator, `Auto-HLS`, is proposed to directly generate synthesizable C code of the DNN models, to conduct latency/resource estimation and FPGA accelerator generation.
- We demonstrate our co-design approach on an **object detection task** targeting a PYNQ-Z1 embedded FPGA. DNN models are searched and mapped to the board with the state-of-the-art performance regarding accuracy, speed, and power efficiency.

### Some Knowledge About DNN

- DNN design is conducted either manually by machine learning experts or automatically by Neural Architecture Search (NAS) such as recursive neural networks (RNN) and reinforcement learning.
- quantization and model compression are used to reduce DNN model size
- latency-directed resource allocation and fine-grained pipeline architecture are proposed to deliver low latency during DNN inference

### FPGA/DNN Co-Design

#### Design Space

- DNN design
  - the number and types of layers
  - the number of input/output channels
  - residual connections
  - concatenations
- FPGA accelerator
  - IP instance categories
  - IP reuse strategies
  - quantization schemes
  - parallel factors
  - data transfer behaviors
  - buffer sizes

For FPGA accelerator, use IP-based design strategy. Each IP supports a basic DNN layer type (e.g. Conv, Pooling), which must be instantiated and configured if the DNN model contains such type of layer.

<img src="https://raw.githubusercontent.com/SingularityKChen/PicUpload/master/img/20200208162333Key%20Variables%20for%20FPGA/DNN%20Co-Design.png" alt="Key Variables for FPGA/DNN Co-Design" style="zoom: 50%;" />

#### Co-Design Flow

- Co-Design Step 1: **Building block and DNN modeling**. Capture the **hardware latency** and **resource utilization** of DNN building blocks and hardware IP pool.

- Co-Design Step 2: **Building block selection**. 
  - `Auto-DNN` performs both coarse and fine-grained evaluations of the building blocks regarding three most important features: **latency**, **resource utilization** and **accuracy**. 
  - Based on the evaluation, building blocks on the Pareto curve will be selected for further DNN exploration.
- Co-Design Step 3: **Hardware-aware DNN search and update**. 
  - Given selected building blocks, `Auto-DNN` explores the DNNs under given resource and latency constraints by using stochastic coordinate descent (SCD). 
  - DNNs output from SCD are passed to `Auto-HLS` to get more precise performance and resource results
  - Are fed back to SCD for update. 
  - The generated DNNs that meet performance and resource requirements are output for training and fine-tuning.

#### Four Components

![The overall FPGA/DNN co-design flow](https://raw.githubusercontent.com/SingularityKChen/PicUpload/master/img/20200208163553The%20overall%20FPGA/DNN%20co-design%20flow.png)

- `Bundle-Arch` as a hardware-aware DNN template (green)
- `Tile-Arch` as a low-latency accelerator template (yellow)
- `Auto-DNN` for DNN exploration (blue), works as the primary component and outputs DNN models
- `Auto-HLS ` for FPGA accelerator synthesizable C code generation (pink), outputs the corresponding FPGA implementations of the DNN models

## BitFlow: Exploiting Vector Parallelism for Binary Neural Networks on CPU

<img src="https://raw.githubusercontent.com/SingularityKChen/PicUpload/master/img/20200208164347%20BitFlow%20System%20Overview.png" alt="BitFlow System Overview" style="zoom: 50%;" />

- Gemm-Level Optimization: fuse binarization, bit-packing, and transposition into a single operation
- Operator-Level Optimization: BitFlow abandons the conventional image-to-column method due to low arithmetic intensity and unfriendly pattern for bitwise operations when applied to binary convolution, and introduces a new class of algorithm named `Pressed-Conv` using **locality-aware layout** and **vector parallelism** for efficient binary convolution. It uses **SIMD** efficiently with a general vector execution scheduler.
- Network-Level Optimization: compared with full precision DNN, BNN introduces binarization and bit-packing stages.
  - conduct binarization and bit-packing of weights during network initialization
  - pre-allocate all the memory needed for storing the output
  - intermediate results by analysis of the neural network as a static computational graph

### Neural Network Compression and Acceleration Methods and Background of BNNs

#### Network Compression and Acceleration

+ Network Pruning: remove non-informative neural connections. the parameters below a certain threshold are removed to form a sparse network, and then the network is re-trained for the remaining connections.
+ Network Quantization
+ Binary Neural Networks: Computationally intensive Floating-point Multiply and Add operations (FMAs) are replaced by `XOR` (for multiplications) and `bit-count` (for accumulations), enabling significant computational speedup.

#### Convolution Operation
+ Unfold
+ Gemm

![](https://raw.githubusercontent.com/SingularityKChen/PicUpload/master/img/20200208221515.png)

### Efficient binary neural network operators

Weights and activations are `{âˆ’1, +1}`, but at the hardware level they must be encoded as `{0, 1}`.

#### Limits of Image-to-column for Binary Convolution

+ Overhead brought by the unfolding procedure, which reduces the maximum achievable fraction of the **intrinsic Arithmetic Intensity** (AIT) of the convolution operation. Arithmetic Intensity (AIT) refers to the ratio of the number of arithmetic operations to the number of memory operations in a computation. The unfolding procedure increases the size of the input while the unfolded input need to be stored before the Gemm, doubling the number of memory accesses to the unfolded input
+ after the input tensor is unfolded into a matrix of $M Ã—N$, $N$ wonâ€™t be multiple of 32 in most cases, making bit-packing and SIMD execution inefficient.

#### Pressed-Conv Algorithm

Pressed-Conv conducts bit-packing of input tensor and filters along the channel dimension, thus pressing them by a factor of 32 (or 64, 128, 256, 512), and then computes convolution of the pressed input tensor and filters.

##### Locality-aware Layout

For efficient bit-packing, we adopt $NHWC$ (batch, height, weight, channel) data layout in BitFlow, rather than $NCHW$. According to the layout, the element $A_{h,w,c}$ is found at position $(hW + w)C + c$ in linear memory. Bit packing is performed along the channel dimension, and this enables efficient memory access.

<img src="https://raw.githubusercontent.com/SingularityKChen/PicUpload/master/img/20200208214851Bit-packing.png" alt="Bit-packing" style="zoom: 33%;" />

##### Vector Execution Scheduler

+ When channel dimension is multiple of 512, pack unsigned ints into `m512i` and utilize `AVX512`.
+ When channel dimension is multiple of 256, pack unsigned ints into `m256i` and utilize `AVX256`.
+ When channel dimension is multiple of 128, pack unsigned ints into `m128i` and utilize `SSE`.
+ When channel dimension is multiple of 32, use intrinsic

![Vector Execution Scheduler](https://raw.githubusercontent.com/SingularityKChen/PicUpload/master/img/20200208223911Vector%20Execution%20Scheduler.png)

##### Address Zero Padding

Assume that the output size of a convolution operator is $HÃ—W$ (one channel) without padding, and we want to pad it to $(H+2)Ã—(W+2)$. We pre-allocate memory of size $(H+2)Ã—(W+2)$ for the output, and store the convolution results in the middle $HÃ—W$ part, leaving the marginal part unchanged (zero as initialized).

![Address Zero Padding](https://raw.githubusercontent.com/SingularityKChen/PicUpload/master/img/20200208223357Zero%20Padding.png)

