---
layout: post
title: "2019/12/23-29 weekly review"
description: "the weekly review from 2019/12/23 to 29"
categories: [WeeklyReview, ReadPaper]
tags: [WeeklyReview, Eyeriss, Eyexam, DSH, DNN, EyerissV2]
last_updated: 2020-02-15 17:15:00 GMT+8
excerpt: This review contains some notes of four papers 1) Efficient Processing of Deep Neural Networks" A Tutorial and Survey; 2) Eyeriss v2" A Flexible and High-Performance Accelerator for Emerging Deep Neural Networks; 3) Eyeriss" A Spatial Architecture for Energy-Efficient Dataflow for Convolutional Neural Networks; 4) Eyeriss" A Spatial Architecture for Energy-Efficient Dataflow for Convolutional Neural Networks. Also, it contains a preparatory thinking of Eyeriss V2's for loops.
redirect_from:
  - /2019/12/29/
---

* Kramdown table of contents
{:toc .toc}
# 2019/12/23-29

Last week, I read four Journal Articles related to Deep Learning Accelerator Implementation. Thanks to Shien's recommendation, I learned a lot from these articles as I have a more vivid knowledge of DLA with several in-deep views in some components. 

When I reviewed these four articles, I found that even though I know how to design the hardware architecture, I may not know how to run some DNN models with the accelerator. Letty told me that they need the help of some compilers to translate the models to instructions, but I don't think I could optimize the compiler to compile the program into my custom instructions. So maybe I would not run a complex model if I can not load instructions manually.

And next week, I am supposed to read some papers which describe the method to compress the different types of data in CNN, i.e., QNN, to decrease the movements of data and the storage requirements of its four levels of the memory hierarchy. Besides, Chen Peng has introduced me thinking that routers can also be regards as one of the calculation components in NoC, so maybe the next few weeks, I will improve the dataflow of Eyeriss with this kind of pattern.

---

## Efficient Processing of Deep Neural Networks: A Tutorial and Survey

This article focuses on:

+ processing of DNN inference
+ addressing the efficiency of the CONV layers

Below is a mind graph drew by [Zhou Yongquan](https://www.zhihu.com/people/zhou-yong-quan-37/answers) at [this zhihu answer](https://zhuanlan.zhihu.com/p/101317065).

![](https://raw.githubusercontent.com/SingularityKChen/PicUpload/master/img/20200215171428.jpg)

Or original picture [here](https://pic4.zhimg.com/80/v2-513d9ea769932e57046ace99c5c3c99f_hd.jpg).

### Creating A System for Efficient DNN Processing

+ Applications and specific computations requirements
+ Understand and balance the important system metrics
  + Accuracy
    + Energy
    + Throughput
    + Hardware cost
+ Optimize DNN processing
+ Joint hardware/software co-design

### Temporal Architectures and Spatial Architectures

![Temporal Architectures and Spatial Architectures](https://images-cdn.shimo.im/Y0FJBfLpHro9PDzw/image.png)

#### Temporal Architectures

+ appear mostly in CPUs or GPUs
+ employ a variety of techniques to improve parallelisms such as vectors (SIMD) or parallel threads (SIMT)
+ use a centralized control for a large number of ALUs
+ can only fetch data from the memory hierarchy
+ cannot communicate directly with each other
+ reduce the number of multiplications to increase throughput

#### Spatial Architectures

+ use dataflow processing can pass data from one to another directly
+ ALU can have its control logic and local memory
+ how dataflows can increase data reuse from low-cost memories in the memory hierarchy to reduce energy consumption

#### Apply Computational Transforms to The Data

+ Fast Fourier Transform (FFT) from O( $N^2_o$ $N^2_f$ ) to O($N^2_o$ $log_2N_o$) 
+ Winograd’s algorithm
+ Strassen’s algorithm from O($N^3$) to O($N^{2.807}$)

#### Energy-Efficient Dataflow for Accelerators

Each MAC requires three memory reads:

+ filter weight
+ fmap activation
+ partial sum

And one memory write: updated partial sum.

Different Dataflow:

+ Weight Stationary *(WS)*
+ Output Stationary *(OS)*
+ No Local Reuse *(NLR)*
+ Row Stationary *(RS)*

### Near-Data Processing

We are supported to reduce data movement by moving compute and data closer. For example, 

+ Integrate the computation into the memory itself
+ Bring the computation into the sensor where the data are first collected

#### DRAM

Avoid off-chip access by using high-density memories such as DRAMs. which can store tens of megabytes of weights and activations on-chip.

Also, with the help of through-silicon vias *(TSVs)*, or 3-D memory, and HMC, HBM, the DRAM can be stacked on the top of the chip.

#### SRAM

Bring the compute into the memory.

#### Non-volatile Resistive Memories

+ resistor’s conductance as the weight
+ the voltage as the input
+ the current as the output
+ the addition is done by Kirchhoff’s current law

But it has some cons:

+ suffers from the reduced precision as it needs ADC/DAC
+ the array size is limited by the wires
+ the IR drop along wire can degrade the read accuracy

#### Sensors

Need to move the computation into the analogy domain to avoid using the ADC within the sensor.

### Co-Design of DNN Models And Hardware

#### Reduce Precision

And we can apply different precisions into weights and activations, different layers.

Reduce the precision of operations and operands:

+ fixed point instead of floating-point
+ reducing the bit-width: uniform quantization
+ nonuniform quantization: map data to a smaller set of quantization levels
  + log quantization
    + learned quantization
+ weight sharing

#### Reduced Number of Operations and Model Size

Reduce the number of operations and model size:

+ compression: exploiting activation statistics
+ network pruning
+ compact network architectures: replace a large filter with a series of small filters

### Benchmarking Metrics for DNN Evaluation and Comparison

#### Metrics for DNN Models

+ The accuracy
+ The network architecture of the model including the number of layers, filter sizes, number of filters and number of channels
+ The number of weights: impact the storage requirement
+ The number of MACs: potential throughput

#### Metrics for DNN Hardware

+ The power  and energy consumption
+ The latency and throughput
+ The cost of the chip: the size and type of memory, the amount of control logic
+ For an FPGA, 
  + the specific device
    + the utilization of resources such as:
      + DSP
        + BRAM
        + LUT
        + FF
    + performance density
      + GOPs/slice

## Eyeriss v2: A Flexible and High-Performance Accelerator for Emerging Deep Neural Networks

This article describes:

+ a performance analysis framework named *`Eyexam`* which provides a seven-step process to systematically identify the source of performance loss and can be used to develop a set of `roofline models` to quantitatively identify the source of performance loss in DNN processors;
+ a DNN accelerator named *`Eyeriss v2`* which uses a hierarchical `NoC` that allows the system to exploit spatial data reuse for large DNNs and deliver high bandwidth for compact DNNs with the scalability;

### Changes in Performance and Flexibility

#### Two Bad Ways in Widely Varying Data Reuse

+ depend on data reuse in certain data dimensions
  + the spatial accumulation array architecture relies on both output and input channels;
    + the temporal accumulation array architecture relies on another set of data dimensions;
+ lower data reuse need a higher data bandwidth

#### To Build a Truly Flexible DNN Processor

+ the dataflow relies on certain data dimensions for data reuse
  + low utilization of the parallelism when those data dimensions diminish
+ the NoC and its corresponding memory hierarchy
  + high bandwidth and low spatial data reuse;
    + low bandwidth and high spatial data reuse scenarios;
    + instead of being adaptive to the specific condition of the workload;

### Eyexam

#### Two Main Factors that Affect Performance

+ the number of active PEs due to the mapping determined by the dataflow;
+ the utilization of active PEs based on whether the NoC has sufficient bandwidth to deliver data to PEs to keep them active;

This is a example dataflow for a 1D CONV:

```c
#include <stdio.h>

int main()
{
 int R2, R1, R0, E2, E1, E0;
 // R0, R1, R2 related to filter
 R2 = 2;//chnl of filter, C, sum of one chnl ofmap
 R1 = 3;//height of filter, R, psum of one row ofmap
 R0 = 4;//width of filter, S, ppsum of one ofmap
 // E0, E1, E2 related to ofmap
 E2 = 2;//chnl of ofmap or number of filters, M, sums in different chnl ofmaps
 E1 = 4;//height of ofmap, E, psum of one chnl ofmap
 E0 = 5;//width of ofmap, F, ppsum of one row ofmap
 int inx_o, inx_i, inx_w;
 for (int e2 = 0; e2 < E2; e2++) {
  for (int r2 = 0; r2 < R2; r2++) {
   for (int e1 = 0; e1 < E1; e1++) {
    for (int r1 = 0; r1 < R1; r1++) {
     for (int e0 = 0; e0 < E0; e0++) {
      for (int r0 = 0; r0 < R0; r0++) {
       inx_o = e2*E1*E0+e1*E0+e0;
       inx_i = e2*E1*E0+e1*E0+e0 + r2*R1*R0+r1*R0+r0;
       inx_w = r2*R1*R0+r1*R0+r0;
       printf("O[%d] += I[%d] * W[%d]\t", inx_o, inx_i, inx_w);
       //printf("O[e2*E1*E0+e1*E0+e0] = O[%d*E1*E0+%d*E0+%d]\n", e2, e1, e0);
       /* code */
      }
      /* code */
      //printf("----------- r0 for loop finished -----------\n");
      printf("\n");
     }
     /* code */
     printf("*********** e0 for loop finished ***********\n");
    }
    /* code */
    printf("+++++++++++ r1 for loop finished +++++++++++\n");
   }
   /* code */
   printf("=========== e1 for loop finished ===========\n");
  }
  /* code */
  printf("@@@@@@@@@@@ r2 for loop finished @@@@@@@@@@@\n");
 }
 return 0;
}

```

+ inner two `for` loops represent the temporal processing and `SPad` accesses within a `PE`;
+ the outer two `for` loops represent the temporal processing of multiple passes across the `PE` array and GLB accesses;
+ the two `parallel-for`s represent the distribution of computation across multiple `PE`s;

#### Seven steps to analyse the performance of an architecture

+ layer shape and size: the number of `MAC`s in the layer
+ dataflow: the maximum parallelism of the dataflow
+ number of `PE`s: the theoretical peak performance. Two shape fragmentation:
  + spatial mapping fragmentation: `R` or `R1` is smaller than the number of `PE`s => some are completely idle
    + temporal mapping fragmentation: `R` is not an integer multiple of `PE`s => some are not always active
+ physical dimensions of the `PE` array: the run-time utilization of `PE`s due to shape fragmentation for each dimension
+ storage capacity: reduce the number of active `PE`s due to storage of intermediate data. e.g., limited storage for `psum`s => the limited number of weights be processed in parallel => limited number of active `PE`s that can operate in parallel
+ data bandwidth: insufficient average bandwidth to active `PE`s. The performance will be bandwidth-limited when the operational intensity is lower than the inflection point
+ varying data access patterns: insufficient instant bandwidth to active `PE`s

![the roofline model](https://images-cdn.shimo.im/C9YNzwIHSE87b2sJ/image.png)

![the impact of Eyexam steps on the roofline model](https://images-cdn.shimo.im/OJ7PwTSeEwIakuk9/image.png)

![summary of steps in Eyexam](https://images-cdn.shimo.im/baEhAdv31Z0zD03y/image.png)

#### Performance Analysis Results for DNN Processors and Workloads

+ simply increasing hardware resources is not sufficient to achieve a higher performance
+ the dataflow has to be flexible enough to deal the diminished reuse available in any data dimensions
+ the NoC design should meet the worst-case bandwidth requirement for every data type
+ the NoC design should aim to exploit data reuse to minimize the number of GLB accesses

### Flexible Dataflow

The new dataflow named *`Row-Stationary Plus`* supports data tiling from all dimensions to fully utilize the PE array.

+ additional loops *`g1`* and *`n1`* are added at the NoC level to provide more options to parallelize different dimensions of data
+ allow data to be tiled from multiple dimensions and mapped in parallel onto the same `PE` array dimension
+ the data tile from the same dimension can also be mapped spatially onto different physical dimensions to fully utilize the `PE` array
+ allow tile in data dimension *`R`* with loop *`r1`*
+ an additional loop *`e0`* is added at the `SPad` level to create more reuse of weights in the `SPad`.

![The definition of the Row-Stationary Plus (RS+) dataflow](https://images-cdn.shimo.im/Lr26hZAgvUcKEqu7/image.png)

```c
inx_o = g3*G2*G1 + g2*G1 + g1
				+ n3*N2*N1*N0 + n2*N1*N0 + n1*N0 + n0
				+ m3*M2*M1*M0 + m2*M1*M0 + m1*M0 + m0
				+ e0
				+ f3*F2*F1*F0 + f2*F1*F0 + f1*F0 + f0;
inx_i = g3*G2*G1 + g2*G1 + g1
				+ n3*N2*N1*N0 + n2*N1*N0 + n1*N0 + n0
				+ e0
				+ f3*F2*F1*F0 + f2*F1*F0 + f1*F0 + f0
				+ c2*C1*C0 + c1*C0 + c0
				+ r0
				+ s2*S1 + s1;
inx_w = g3*G2*G1 + g2*G1 + g1
				+ m3*M2*M1*M0 + m2*M1*M0 + m1*M0 + m0
				+ c2*C1*C0 + c1*C0 + c0
				+ r0
				+ s2*S1 + s1;

inx_o[0] = m0 + e0*M0 + n0*E0*M0 + f0*N0*E0*M0;
inx_w[0] = m0 + c0*M0 + r0*C0*M0;
inx_i[0] = c0 + r0*C0 + e0*R0*C0 + n0*E0*R0*C0 + f0*N0*E0*R0*C0;
```



### Flexible and Scalable NoC

A new NoC named *`hierarchical mesh`* is designed to adapt to a wide range of bandwidth requirements.

#### The Pros and Cons of Different NoC Implementations

+ Broadcast Network: high reuse but low bandwidth
+ Unicast Network: high bandwidth but low reuse
+ All-to-All Network: implementation cost and energy consumption increase quadratically 

![The pros and cons of different NoC implementations](https://images-cdn.shimo.im/otl1ciltLJkHZZql__thumbnail)

#### Architecture and Work Models of Hierarchical Mesh Network

![Architecture of Hierarchical Mesh Network](https://images-cdn.shimo.im/KGUO8tm73RYl12vk/image.png__thumbnail)

![Work Models of Hierarchical Mesh Network](https://images-cdn.shimo.im/xxbVBVUFHdkow9E3/image.png__thumbnail)

#### Considerations

The size of clusters:

+ what is the bandwidth required from the source cluster in the worst case, i.e., unicast mode;
+ what is the bandwidth required in between the router clusters in the worst case, i.e., interleaved multicast mode;
+ what is the tolerable cost of the all-to-all network;

The tile size of the mapped data dimensions:

+ the cluster size
+ the number of clusters

## Eyeriss: A Spatial Architecture for Energy-Efficient Dataflow for Convolutional Neural Networks

Compared to the [Eyeriss v2](#eyeriss-v2-a-flexible-and-high-performance-accelerator-for-emerging-deep-neural-networks), this article provides a more detailed explanation of *Row Stationary*, a baseline storage area for a given number of PEs and the energy cost estimation for RS reuse pattern.

---

This article proposed RS dataflow which can adapt to different CNN shape configurations and reduces all types of data movement through maximally utilizing the processing engine (PE) local storage, direct inter-PE communication and spatial parallelism.

Also, an analysis framework that compares energy cost under the same hardware area and processing parallelism constraints.

### Framework for Energy Efficiency Analysis

#### Storage Area

The baseline storage area for a given number of PEs is calculated as

```
#PE×Area(512B RF)+Area((#PE×512B) global buffer).
```

![](https://images-cdn.shimo.im/H6Ir275vy2cj9SXP/image.png)

#### Input Data Access Energy Cost

Input data access energy cost estimation:

```
a×EC(DRAM)+ab×EC(global buffer)+abc×EC(array)+abcd×EC(RF)
```

![Example of input data](https://images-cdn.shimo.im/4HIcPqN7ZJYtzTZu/image.png)

#### Psum Accumulation Energy Cost

Psum accumulation energy cost estimation:

```
(2a−1)×EC(DRAM)+2a(b−1)×EC(global buffer)+ab(c−1)×EC(array)+2abc(d−1)×EC(RF)
```

![Example of Psum accumulation](https://images-cdn.shimo.im/YpjPqAj7BIkUqJOS/image.png)

where the `EC(*)` is shown as follows:

![NORMALIZED ENERGY COST RELATIVE TO A MAC OPERATION EXTRACTED FROM A COMMERCIAL 65NM PROCESS](https://images-cdn.shimo.im/QQxUYwRr9kEkrpyE/image.png)



## Eyeriss: An Energy-Efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks

Compared to the [Eyeriss v2](#eyeriss-v2-a-flexible-and-high-performance-accelerator-for-emerging-deep-neural-networks) and [Eyeriss energy](#eyeriss-an-energy-efficient-reconfigurable-accelerator-for-deep-convolutional-neural-networks), this article provides a more detailed explanation on the system architecture of the NoC communication as well as mapping of the PE sets. Also, it introduces an RLC compressed form to reduce the data size as related components in PE architecture.

Plus, this article mentions CNN biases, but there is nothing related to the biases in the architecture it describes.

---

This article introduces the way to reduce the cost of data movement by exploiting data reuse in a multilevel memory hierarchy.  Also, techniques such as compression and data-adaptive processing are introduced to save both memory bandwidth and processing power.

### Main Features of Eyeriss

+ A four-level memory hierarchy. Data movement can exploit low-cost levels:
  + the PE scratch pads
    + the inter-PE communication
    + the large on-chip global buffer (GLB)
    + the off-chip DRAM.
+ A CNN dataflow, called Row Stationary (RS), that reconfigures the spatial architecture to map the computation of a given CNN shape and optimize for the best energy efficiency.
+ A network-on-chip (NoC) architecture that uses both multicast and point-to-point single-cycle data delivery to support the RS dataflow.
+ Run-length compression (RLC) and PE data gating that exploits the statistics of zero data in CNNs to further improve energy efficiency.

This is the architecture of Eyeriss system:

![Eyeriss system architecture](https://images-cdn.shimo.im/KUn0Ny1sa6wMuMuy/image.png)

### System Control and Configuration

The accelerator has two levels of control hierarchy. 

+ The top-level control coordinates:
  + traffic between the off-chip DRAM and the GLB through the asynchronous interface;
    + traffic between the GLB and the PE array through the NoC;
    + operation of the RLC CODEC and ReLU module. 
+ The lower-level control consists of control logic in each PE, which runs independently of each other.

The size:

+ spad capacity depends only on the filter row size (S)
  but not the ifmap row size (W), and is equal to:
  	+ S for a row of filter weights; 
  	+ S for a sliding window of ifmap values;
  	+ 1 for the psum accumulation.

+ the height and the width of the PE set are equal to the number of filter rows (R) and ofmap rows (E), respectively.

![Mapping of the PE sets on the spatial array of 168 PEs for the CONV layers in AlexNet](https://images-cdn.shimo.im/JlEPRdIC1rU50uoT/image.png)

### Exploit Data Statistics

data statistics of CNN is explored to:

+ reduce DRAM accesses using compression, which is the most energy-consuming data movement per access, on top of the optimized dataflow;
+ skip the unnecessary computations to save processing power

#### RLC

RLC is used in Eyeriss to exploit the zeros in fmaps and save DRAM bandwidth.

![an example of RLC encoding](https://uploader.shimo.im/f/Ir9k6AuBWP0NWomF.png!thumbnail)

#### NoC

Requirements of the NoC architecture:

+ support the data delivery patterns used in the RS dataflow; should be taken care of:
  + different convolution strides (U) result in the ifmap delivery, skipping certain rows in the array;
    + a set is divided into segments that are mapped onto different parts of the PE array;
    + multiple sets are mapped onto the array simultaneously and different data is required for each set
+ leverage the data reuse achieved by the RS dataflow to further improve energy efficiency;
+ provide enough bandwidth for data delivery to support the highly parallel processing in the PE array.

Three different types of networks:

+ Global Input Network: is optimized for a single-cycle multicast from the GLB to a group of PEs that receive the same filter weight, ifmap value, or psum. The challenge is that the group of destination PEs varies across layers due to the differences in data type, convolution stride, and mapping.
+ Global Output Network: The GON is used to read the psums generated by a processing pass from the PE array back to the GLB.
+ Local Network: pass the psums from two consecutive rows of the same column

![Architecture of the GIN](https://images-cdn.shimo.im/RrYcTIHJCF4DOYXc/image.png)

#### Processing Element and Data Gating

The datapath is pipelined into three stages: one stage for spad access, and the remaining two for computation.

An extra 12-b Zero Buffer is used to record the position of zeros in the ifmap spad. If a zero ifmap value is detected from the zero buffer, the gating logic will disable the read of the filter spad and prevent the MAC datapath from switching.

![PE architecture](https://images-cdn.shimo.im/MPBbaUOFLNEcqibp/image.png)

## Some Troubles

### Scalac_2.12.4:4.2.3

I tried run verilator simulation with the default command

```bash
make CONFIG=SmallBoomAndRocketConfig BINARY=mt-vvadd.riscv run-binary
```

But got the following error:

```
[warn]     Note: Unresolved dependencies path:
[error] sbt.librarymanagement.ResolveException: Error downloading org.scalameta:semanticdb-scalac_2.12.4:4.2.3
[error]   Not found
[error]   Not found
[error]   not found: /home/singularity/.ivy2/local/org.scalameta/semanticdb-scalac_2.12.4/4.2.3/ivys/ivy.xml
[error]   not found: https://repo1.maven.org/maven2/org/scalameta/semanticdb-scalac_2.12.4/4.2.3/semanticdb-scalac_2.12.4-4.2.3.pom
[error]   not found: https://oss.sonatype.org/content/repositories/snapshots/org/scalameta/semanticdb-scalac_2.12.4/4.2.3/semanticdb-scalac_2.12.4-4.2.3.pom
[error]   not found: https://oss.sonatype.org/content/repositories/releases/org/scalameta/semanticdb-scalac_2.12.4/4.2.3/semanticdb-scalac_2.12.4-4.2.3.pom
```

When I got into the website it mentioned `https://repo1.maven.org/maven2/org/scalameta/` I could only found the version `4.2.3` in `https://repo1.maven.org/maven2/org/scalameta/semanticdb-scalac_2.13.1/4.2.3/`.

Firstly, I tried to change the Scala version to `2.13.1` but occurred some syntax errors when I tried to `sbt publishLocal` for `firrtl`, so I had to give up that way.

Then, I wanted to find why the default Scala version is `2.12.4` though I had set that to `2.12.1` in `build.sbt`. Then I found the default setting was in `variables.mk:143`, so I changed it to `2.12.10`, which is the same to that in `firrtl`.

### No found: firrlt object

After that, I countered with the old trouble: `not found: object firrtl`

```
[error] /home/singularity/chipyard/tools/chisel3/src/main/scala/chisel3/ChiselExecutionOptions.scala:7:8: not found: object firrtl
[error] import firrtl.{AnnotationSeq, ExecutionOptionsManager, ComposableOptions}
[error]        ^
[error] /home/singularity/chipyard/tools/chisel3/src/main/scala/chisel3/ChiselExecutionOptions.scala:21:44: not found: type ComposableOptions
```

I tried to re-do `publishLocal` in almost all components in `chipyard/tools` but failed.

Then, Jiuyang told me that I was supposed to `sbt publishLocal` then `sbt update` in these three directories in order:

+ firrtl
+ chisel3
+ rocket-chip

But while I tried to `publishLocal` under rocket-chip, I found the version of firrtl it need was 1.2 while I only had 1.3 version. So I re-do this flow with the correctness of `version := "1.2-SNAPSHOT"`  in `build.sbt` under firrtl.

Although I published the rocket-chip, I could not build chipyard successfully.