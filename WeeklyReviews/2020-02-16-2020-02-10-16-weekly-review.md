---
layout: post
title: "2020/02/10-16 weekly review"
last_updated: 2020-02-15 17:14:00 GMT+8
description: "the weekly review from 2020/02/10 to 16"
categories: [WeeklyReview, ReadPaper]
tags: [WeeklyReview, Hwacha, RoCC, Chisel, DMA, ML4HPC, HPML, HPC]
excerpt: This review contains one way to think matrix multiply, one Chisel class named DataMirror which can monitor the details of ports, and a discussing of how can RoCC accelerator communicate with L2 cache. Also, I attached my recent survey on AI4HPC, including three papers 1) A Modular Benchmarking Infrastructure for High-Performance and Reproducible Deep Learning; 2) HPC AI500" A Benchmark Suite for HPC AI Systems; 3) A Modular Benchmarking Infrastructure for High-Performance and Reproducible Deep Learning; and some other presentation slides.
redirect_from:
  - /2020/02/16/
---

* Kramdown table of contents
{:toc .toc}
# 2020/02/10-16

This week, I continued my survey at `AI for HPC` and the implementation of Eyeriss V2.

I found a visualization tool named [`Netron`](https://github.com/lutzroeder/netron) to translate the Neural Network from `.onnx` etc. files to graphs.


## Matrix Multiply

### A Geometrical View

![A Geometrical View of Matrix Multiply](https://raw.githubusercontent.com/SingularityKChen/PicUpload/master/img/20200211220013A%20Geometrical%20View%20of%20Matrix%20Multiply.png)

![](https://raw.githubusercontent.com/SingularityKChen/PicUpload/master/img/20200211220735A%20Geometrical%20View%20of%20Matrix%20Multiply.png)

## Chisel Syntax

### `DataMirror`

Defined [there](https://github.com/freechipsproject/chisel3/blob/master/chiselFrontend/src/main/scala/chisel3/Data.scala#L126).

```scala
object DataMirror {
  def widthOf(target: Data): Width = target.width
  def specifiedDirectionOf(target: Data): SpecifiedDirection = target.specifiedDirection
  def directionOf(target: Data): ActualDirection = {
    requireIsHardware(target, "node requested directionality on")
    target.direction
  }

  // Returns the top-level module ports
  // TODO: maybe move to something like Driver or DriverUtils, since this is mainly for interacting
  // with compiled artifacts (vs. elaboration-time reflection)?
  def modulePorts(target: BaseModule): Seq[(String, Data)] = target.getChiselPorts

  // Returns all module ports with underscore-qualified names
  def fullModulePorts(target: BaseModule): Seq[(String, Data)] = {
    def getPortNames(name: String, data: Data): Seq[(String, Data)] = Seq(name -> data) ++ (data match {
      case _: Element => Seq()
      case r: Record => r.elements.toSeq flatMap { case (eltName, elt) => getPortNames(s"${name}_${eltName}", elt) }
      case v: Vec[_] => v.zipWithIndex flatMap { case (elt, index) => getPortNames(s"${name}_${index}", elt) }
    })
    modulePorts(target).flatMap { case (name, data) =>
      getPortNames(name, data).toList
    }
  }
}

// usage
DataMirror.modulePorts(adder).foreach { case (name, port) => {
  println(s"Found port $name: $port")
}}
assert(DataMirror.directionOf(flippedVec.head.a) == Direction.Output)

val specifiedDirs = Seq(
    a.fizz.foo -> SpecifiedDirection.Input,
    a.fizz.bar -> SpecifiedDirection.Output,
    a.fizz -> SpecifiedDirection.Unspecified,
    a.buzz.foo -> SpecifiedDirection.Input,
    a.buzz.bar -> SpecifiedDirection.Output,
    a.buzz -> SpecifiedDirection.Flip
)
val actualDirs = Seq(
    b.fizz.foo -> Direction.Input,
    b.fizz.bar -> Direction.Output,
    b.fizz -> Direction.Bidirectional(Direction.Default),
    b.buzz.foo -> Direction.Output,
    b.buzz.bar -> Direction.Input,
    b.buzz -> Direction.Bidirectional(Direction.Flipped)
)
for ((data, dir) <- specifiedDirs) {
    DataMirror.specifiedDirectionOf(data) shouldBe (dir)
}
for ((data, dir) <- actualDirs) {
    DataMirror.directionOf(data) shouldBe (dir)
}

```

## Chipyard

### How to Access DMA

From [this post](https://www.chiselchina.com/forum/topic/19/rocc-%E5%8A%A0%E9%80%9F%E5%99%A8%E8%AE%BE%E8%AE%A12-rocc-l1dcache-%E5%92%8C-dma-%E8%AE%BF%E5%AD%98%E5%AE%9E%E7%8E%B0) at [chisel China](https://www.chiselchina.com/forum/), the `RoCC` accelerators can communicate with main memory via DMA with `TileLink`'s help. I got that post from [this issue blog](https://github.com/meton-robean/ResearchNote/issues/30).

I found [one DMA project](https://github.com/zhemao/riscv-dma3) written in Chisel with TileLink protocol. So maybe I will utilize this design directedly.

### How to Access Level 2 Cache

Although [an earlier document](https://docs.google.com/document/d/1CH2ep4YcL_ojsa3BVHEW-uwcKh1FlFTjH_kg5v8bxVw/edit#) shows that `RoCC` accelerators can communicate with L2 memory via `Uncached Tile Link`\(UTL\), but we can not find this port now from the class [`RoCCIO`](https://github.com/chipsalliance/rocket-chip/blob/master/src/main/scala/tile/LazyRoCC.scala#L51). 

![Old RoCC Interface](https://raw.githubusercontent.com/SingularityKChen/PicUpload/master/img/20200214222739RoCC.png)

From [the discussing](https://groups.google.com/a/groups.riscv.org/forum/#!searchin/hw-dev/rocc%7Csort:date/hw-dev/_hcbPkXrGdk/7xFZiDe0DgAJ) in HW Dev Google Group, one said that we can learn the details from `Hwacha` and `Gemmini`, so I fetched some details in those projects.

#### `Hwacha`

From [the document `Hardware Acceleration for Memory to Memory Copies`](https://www2.eecs.berkeley.edu/Pubs/TechRpts/2017/EECS-2017-2.pdf), we can see that the submodules of [Hwacha](https://github.com/ucb-bar/hwacha): `SMU`, `VMU`, `VRU` and one L1 cache connect to `L1-to-L2 TileLink Crossbar`, which might be `SystemBus` now. So  maybe we can find some common ports in those modules and then we can get the method to communicate with L2 cache.

![Hwacha Design](https://raw.githubusercontent.com/SingularityKChen/PicUpload/master/img/20200214223405Hwacha.png)

[`Hwacha.smu.scala`](https://github.com/ucb-bar/hwacha/blob/master/src/main/scala/smu.scala)

```scala
class SMUReq(implicit p: Parameters) extends SMUBundle()(p)
  with SMUData {
  val fn = new SMUFn
  val addr = UInt(width = bVAddrExtended)
  val status = new freechips.rocketchip.rocket.MStatus
}

class SMUResp(implicit p: Parameters) extends SMUBundle()(p)
  with SMUData {
  val store = Bool()
}

class SMUIO(implicit p: Parameters) extends HwachaBundle()(p) {
  val req = Decoupled(new SMUReq)
  val resp = Decoupled(new SMUResp).flip
  val confirm = Bool(INPUT)
}
```

[`Hwacha.vmu-memif.scala`](https://github.com/ucb-bar/hwacha/blob/master/src/main/scala/vmu-memif.scala)

```scala
class VMUMemReq(implicit p: Parameters) extends VMUMemOp
  with VMUTag with VMUData {
  val mask = UInt(width = tlDataBytes)
  val pred = Bool()
}

class VMUMemResp(implicit p: Parameters) extends VMULoadData()(p) {
  val store = Bool()
}

class VMUMemIO(implicit p: Parameters) extends VMUBundle()(p) {
  val req = Decoupled(new VMUMemReq)
  val resp = Decoupled(new VMUMemResp).flip
}
```

[`Hwacha.types-vmu.scala`](https://github.com/ucb-bar/hwacha/blob/master/src/main/scala/types-vmu.scala)

```scala
abstract class HwachaBundle(implicit val p: Parameters) extends ParameterizedBundle()(p)
  with UsesHwachaParameters

abstract class VMUBundle(implicit p: Parameters)
  extends HwachaBundle()(p) with VMUParameters

trait VMUAddr extends VMUBundle {
  val addr = UInt(width = bPAddr)
}

trait VMUMemOp extends VMUAddr {
  val fn = new VMUMemFn
}

trait VMUTag extends VMUBundle {
  val tag = UInt(width = bVMUTag)
}

trait VMUData extends VMUBundle {
  val data = Bits(width = tlDataBits)
}

class VMULoadData(implicit p: Parameters) extends VMUData with VMUTag
```

## AI For HPC Benchmarks

The screenshot bellow shows the materials related to HPC benchmarks. Although most of them are `HPC for AI` instead of `AI for HPC`.

![Current Benchmarks](https://raw.githubusercontent.com/SingularityKChen/PicUpload/master/img/20200216190127CurrentBenchmark.png)

### [Deep Learning on HPC: Performance Factors and Lessons Learned](http://www.benchcouncil.org/bench19/file/slides/invite4.pdf)

#### Scale up vs. Scale out

![Scale up vs. Scala out](https://raw.githubusercontent.com/SingularityKChen/PicUpload/master/img/20200215195757.png)

#### Conclusion

![](https://raw.githubusercontent.com/SingularityKChen/PicUpload/master/img/20200215200136.png)

![](https://raw.githubusercontent.com/SingularityKChen/PicUpload/master/img/20200215200240.png)

### [Implications of Integration of Deep Learning and HPC for Benchmarking](http://dsc.soic.indiana.edu/presentations/BenchCouncilNov19-2019.pptx)

![](https://raw.githubusercontent.com/SingularityKChen/PicUpload/master/img/20200216200055.png)

![ML4HPC & HPC4ML](https://raw.githubusercontent.com/SingularityKChen/PicUpload/master/img/20200216200426.png)

![](https://raw.githubusercontent.com/SingularityKChen/PicUpload/master/img/20200216200450.png)

### [AI for HPC and HPC for AI Workflows: The Differences, Gaps and Opportunities with Data Management](https://www.sc-asia.org/2018/wp-content/uploads/2018/03/1_1800_Dr-Rangan-Sukumar.pdf)

![](https://raw.githubusercontent.com/SingularityKChen/PicUpload/master/img/20200216204306.png)

![](https://raw.githubusercontent.com/SingularityKChen/PicUpload/master/img/20200216201413.png)

![](https://raw.githubusercontent.com/SingularityKChen/PicUpload/master/img/20200216201441.png)

![](https://raw.githubusercontent.com/SingularityKChen/PicUpload/master/img/20200216201504.png)

![](https://raw.githubusercontent.com/SingularityKChen/PicUpload/master/img/20200216201602.png)

### [HPC AI500: A Benchmark Suite for HPC AI Systems](http://arxiv.org/abs/1908.02607)

![HPC AI500 Methodology](https://raw.githubusercontent.com/SingularityKChen/PicUpload/master/img/20200216201801.png)

![](https://raw.githubusercontent.com/SingularityKChen/PicUpload/master/img/20200216201831.png)

![](https://raw.githubusercontent.com/SingularityKChen/PicUpload/master/img/20200216201855.png)

![](https://raw.githubusercontent.com/SingularityKChen/PicUpload/master/img/20200216201931.png)

### A Modular Benchmarking Infrastructure for High-Performance and Reproducible Deep Learning

![Components in Deep Learning](https://raw.githubusercontent.com/SingularityKChen/PicUpload/master/img/20200216202234.png)

This benchmark utilize the Open Neural Network Exchange format to store DNN reproducibly. And almost all frameworks support generate this kind of files. So this benchmark could combine them together.

The essence of Deep500 is its layered and modular design that allows to independently extend and benchmark DL procedures related to simple operators, whole neural networks, training schemes, and distributed training. The principles behind this design can be reused to enable interpretable and reproducible benchmarking of extreme-scale codes in domains outside DL.

#### Benchmark Challenges

+ customizability: the ability to seamlessly and effortlessly combine different features of different frameworks and still be able to provide fair analysis of their performance and accuracy.
+ metrics: considering performance, correctness, and convergence in shared- as well as distributed-memory environments. 
  + some metrics can test the performance of both the whole computation and fine-grained elements, for example latency or overhead.
  + others, such as accuracy or bias, assess the quality of a given algorithm, its convergence, and its generalization towards previously-unseen data.
  + combine performance and accuracy (time-to-accuracy) to analyze the tradeoffs.
  + propose metrics for the distributed part of DL codes: communication volume and I/O latency.
+ performance and scalability: the design of distributed benchmarking of DL to ensure high performance and scalability.
+ validation: A benchmarking infrastructure for DL must allow to validate results with respect to several criteria. As we discuss in § V, Deep500 offers validation of convergence, correctness, accuracy, and performance.
+ reproducibility: the ability to reproduce or at least interpret results.

#### Design and Implementation of Deep500

![The design of Deep500](https://raw.githubusercontent.com/SingularityKChen/PicUpload/master/img/20200216203704.png)

The core enabler in Deep500 is the modular design that groups all the required functionalities into four levels: 
+ “Operators”
+ “Network Processing”
+ “Training”
+ “Distributed Training”

Each level provides relevant abstractions, interfaces, reference implementations, validation procedures, and metrics.

![](https://raw.githubusercontent.com/SingularityKChen/PicUpload/master/img/20200216203935.png)

### Intelligent Data Center Architecture to Enable Next Generation HPC AI Platforms

![](https://raw.githubusercontent.com/SingularityKChen/PicUpload/master/img/20200216204531.png)

![](https://raw.githubusercontent.com/SingularityKChen/PicUpload/master/img/20200216204622.png)

